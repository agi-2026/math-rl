# SFT Configuration for Qwen3-4B via Tinker
# Phase 2: Supervised Fine-Tuning on teacher-distilled reasoning traces

model_name: Qwen/Qwen3-4B-Instruct-2507
lora_rank: 64

# Optimizer (linear decay from this LR to 0)
# 5e-4 is the recommended LoRA LR for Qwen3-4B per get_lr()
learning_rate: 5.0e-4

# Training
num_epochs: 3
batch_size: 32          # smaller batches = more gradient steps per epoch
max_length: 4096        # max sequence length (truncate longer)
train_on_what: all_assistant_messages

# Logging & checkpointing
save_every: 20          # save every N steps
